{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGixWp0ccczd"
      },
      "outputs": [],
      "source": [
        "!pip install adapters datasets transformers torch\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, TrainingArguments, EvalPrediction\n",
        "from adapters import AutoAdapterModel, AdapterConfig, AdapterTrainer\n",
        "\n",
        "# 1. Load Model and Tokenizer\n",
        "model_name = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoAdapterModel.from_pretrained(model_name)\n",
        "\n",
        "# 2. Add Sentiment Adapter\n",
        "# This adds the bottleneck layers and a classification head\n",
        "model.add_adapter(\"sentiment_adapter\", config=\"pfeiffer\")\n",
        "model.add_classification_head(\"sentiment_adapter\", num_labels=2)\n",
        "\n",
        "# 3. Freeze base model and activate adapter for training\n",
        "model.train_adapter(\"sentiment_adapter\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Dataset Prep (using IMDB)\n",
        "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
        "def encode_batch(batch):\n",
        "    return tokenizer(batch[\"text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "tokenized_ds = dataset.map(encode_batch, batched=True)\n",
        "\n",
        "# 5. Training Setup\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./adapter_sentiment\",\n",
        "    learning_rate=1e-4, # Adapters usually need a higher LR than full fine-tuning\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = AdapterTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "model.save_adapter(\"./my_final_adapter\", \"sentiment_adapter\")"
      ],
      "metadata": {
        "id": "IplI51uYcz08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from adapters import AutoAdapterModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = AutoAdapterModel.from_pretrained(\"roberta-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Load your custom-trained adapter\n",
        "model.load_adapter(\"./my_final_adapter\")\n",
        "model.set_active_adapters(\"sentiment_adapter\")\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    return \"Positive\" if predictions.item() == 1 else \"Negative\"\n",
        "\n",
        "print(analyze_sentiment(\"This movie was an absolute masterpiece of modern cinema!\"))"
      ],
      "metadata": {
        "id": "WIApCK6Yc5mT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}