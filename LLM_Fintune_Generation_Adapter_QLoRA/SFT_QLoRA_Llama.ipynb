{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFnV8kLl0SGY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# 1. Configuration\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B\" # Requires HF access approval\n",
        "dataset_name = \"GAIR/lima\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Quantization (QLoRA) to save VRAM\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# 3. Load Model and Tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 4. LIMA Formatting Function\n",
        "# LIMA usually has a 'conversations' column which is a list of strings\n",
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['conversations'])):\n",
        "        # Format as: User: {prompt} \\n Assistant: {response}\n",
        "        text = f\"User: {example['conversations'][i][0]}\\nAssistant: {example['conversations'][i][1]}\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n"
      ],
      "metadata": {
        "id": "laqhAUze0bP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. LoRA Adapter Settings\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], # Target all linear layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# 6. Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama-lima-sft\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4, # LIMA recommends a small LR\n",
        "    num_train_epochs=3,   # LIMA is tiny, so few epochs are needed\n",
        "    logging_steps=10,\n",
        "    bf16=True,            # Use bfloat16 if your GPU supports it (A100/H100/3000+)\n",
        "    save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "# 7. Initialize SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=load_dataset(dataset_name, split=\"train\"),\n",
        "    peft_config=peft_config,\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    max_seq_length=1024,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        ")"
      ],
      "metadata": {
        "id": "ptdwQL5Z0dJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "wN1Cuxsm0mwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"your_huggingface_token_here\")"
      ],
      "metadata": {
        "id": "2rlGhiXc0_P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save locally first\n",
        "trainer.save_model(\"./final_lima_adapter\")\n",
        "\n",
        "# Push to the Hub\n",
        "trainer.push_to_hub(\"your-username/llama-3-8b-lima\")"
      ],
      "metadata": {
        "id": "5UDfEHPd1AHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "adapter_id = \"your-username/llama-3-8b-lima\" # Your Hub ID\n",
        "\n",
        "# 1. Load the Base Model (standard 4-bit/8-bit to save memory)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 2. Load the Adapter from the Hub\n",
        "model = PeftModel.from_pretrained(model, adapter_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "# 3. Simple Inference Function\n",
        "def ask_llama(prompt):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    # Apply the Llama-3 chat template\n",
        "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(inputs, max_new_tokens=150, temperature=0.7)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(ask_llama(\"How do I explain quantum physics to a five-year-old?\"))"
      ],
      "metadata": {
        "id": "r0-ZHtLh1GDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This combines the base + adapter into a single standard model\n",
        "merged_model = model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"./llama-3-lima-full\")"
      ],
      "metadata": {
        "id": "KWt4z08E1PNX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}